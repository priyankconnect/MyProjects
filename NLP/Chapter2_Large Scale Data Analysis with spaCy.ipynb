{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b94abd9",
   "metadata": {},
   "source": [
    "In this chapter, you'll use your new skills to extract specific information from large volumes of text. You'll learn how to make the most of spaCy's data structures, and how to effectively combine statistical and rule-based approaches for text analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a4bfe8",
   "metadata": {},
   "source": [
    "Data Structures (1): Vocab, Lexemes and StringStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a06ab43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spaCy\n",
    "import spacy\n",
    "\n",
    "# Create a blank English nlp object\n",
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f26192",
   "metadata": {},
   "source": [
    "Vocab: stores data shared across multiple documents\n",
    "\n",
    "To save memory, spaCy encodes all strings to hash values\n",
    "\n",
    "Strings are only stored once in the StringStore via nlp.vocab.strings\n",
    "\n",
    "String store: lookup table in both directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c6301ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab.strings.add(\"coffee\")\n",
    "coffee_hash = nlp.vocab.strings[\"coffee\"]\n",
    "coffee_string = nlp.vocab.strings[coffee_hash]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973a5ab4",
   "metadata": {},
   "source": [
    "Hashes can't be reversed – that's why we need to provide the shared vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05b74fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raises an error if we haven't seen the string before\n",
    "string = nlp.vocab.strings[3197928453018144401]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb150a0",
   "metadata": {},
   "source": [
    "Look up the string and hash in nlp.vocab.strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e7cb2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hash value: 3197928453018144401\n",
      "string value: coffee\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I love coffee\")\n",
    "print(\"hash value:\", nlp.vocab.strings[\"coffee\"])\n",
    "print(\"string value:\", nlp.vocab.strings[3197928453018144401])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd83a6a",
   "metadata": {},
   "source": [
    "The doc also exposes the vocab and strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a48b399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hash value: 3197928453018144401\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I love coffee\")\n",
    "print(\"hash value:\", doc.vocab.strings[\"coffee\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f35af54",
   "metadata": {},
   "source": [
    "A Lexeme object is an entry in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a53b6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee 3197928453018144401 True\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I love coffee\")\n",
    "lexeme = nlp.vocab[\"coffee\"]\n",
    "\n",
    "# Print the lexical attributes\n",
    "print(lexeme.text, lexeme.orth, lexeme.is_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c6ceb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contains the context-independent information about a word\n",
    "#Word text: lexeme.text and lexeme.orth (the hash)\n",
    "#Lexical attributes like lexeme.is_alpha\n",
    "#Not context-dependent part-of-speech tags, dependencies or entity labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f370edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5439657043933447811\n",
      "cat\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc = nlp(\"I have a cat\")\n",
    "\n",
    "# Look up the hash for the word \"cat\"\n",
    "cat_hash = nlp.vocab.strings[\"cat\"]\n",
    "print(cat_hash)\n",
    "\n",
    "# Look up the cat_hash to get the string\n",
    "cat_string = nlp.vocab.strings[cat_hash]\n",
    "print(cat_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ead7fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380\n",
      "PERSON\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc = nlp(\"David Bowie is a PERSON\")\n",
    "\n",
    "# Look up the hash for the string label \"PERSON\"\n",
    "person_hash = nlp.vocab.strings[\"PERSON\"]\n",
    "print(person_hash)\n",
    "\n",
    "# Look up the person_hash to get the string\n",
    "person_string = nlp.vocab.strings[person_hash]\n",
    "print(person_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e441c6",
   "metadata": {},
   "source": [
    "Data Structures (2): Doc, Span and Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22c11cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an nlp object\n",
    "import spacy\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# The words and spaces to create the doc from\n",
    "words = [\"Hello\", \"world\", \"!\"]\n",
    "spaces = [True, False, False]\n",
    "\n",
    "# Create a doc manually\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00d3940d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Doc and Span classes\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "# The words and spaces to create the doc from\n",
    "words = [\"Hello\", \"world\", \"!\"]\n",
    "spaces = [True, False, False]\n",
    "\n",
    "# Create a doc manually\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "\n",
    "# Create a span manually\n",
    "span = Span(doc, 0, 2)\n",
    "\n",
    "# Create a span with a label\n",
    "span_with_label = Span(doc, 0, 2, label=\"GREETING\")\n",
    "\n",
    "# Add span to the doc.ents\n",
    "doc.ents = [span_with_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ac8f98",
   "metadata": {},
   "source": [
    "Doc and Span are very powerful and hold references and relationships of words and sentences\n",
    "Convert result to strings as late as possible\n",
    "Use token attributes if available – for example, token.i for the token index\n",
    "Don't forget to pass in the shared vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "883138df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy is cool!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Desired text: \"spaCy is cool!\"\n",
    "words = [\"spaCy\", \"is\", \"cool\", \"!\"]\n",
    "spaces = [True, True, False, False]\n",
    "\n",
    "# Create a Doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66bf35d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go, get started!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Desired text: \"Go, get started!\"\n",
    "words = [\"Go\", \",\", \"get\", \"started\", \"!\"]\n",
    "spaces = [False, True, True, False, False]\n",
    "\n",
    "# Create a Doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "496b0416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, really?!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Desired text: \"Oh, really?!\"\n",
    "words = [\"Oh\", \",\", \"really\", \"?\", \"!\"]\n",
    "spaces = [False, True, False, False, False]\n",
    "\n",
    "# Create a Doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52c660bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like David Bowie\n",
      "David Bowie PERSON\n",
      "[('David Bowie', 'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Import the Doc and Span classes\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "words = [\"I\", \"like\", \"David\", \"Bowie\"]\n",
    "spaces = [True, True, True, False]\n",
    "\n",
    "# Create a doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)\n",
    "\n",
    "# Create a span for \"David Bowie\" from the doc and assign it the label \"PERSON\"\n",
    "span = Span(doc, 2, 4, label=\"PERSON\")\n",
    "print(span.text, span.label_)\n",
    "\n",
    "# Add the span to the doc's entities\n",
    "doc.ents = [span]\n",
    "\n",
    "# Print entities' text and labels\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "035a643c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found proper noun before a verb: Berlin\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Berlin looks like a nice city\")\n",
    "\n",
    "# Iterate over the tokens\n",
    "for token in doc:\n",
    "    # Check if the current token is a proper noun\n",
    "    if token.pos_ == \"PROPN\":\n",
    "        # Check if the next token is a verb\n",
    "        if doc[token.i + 1].pos_ == \"VERB\":\n",
    "            print(\"Found proper noun before a verb:\", token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968308e2",
   "metadata": {},
   "source": [
    "Word vectors and semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78677d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spaCy can compare two objects and predict similarity\n",
    "#Doc.similarity(), Span.similarity() and Token.similarity()\n",
    "#Take another object and return a similarity score (0 to 1)\n",
    "#Important: needs a pipeline that has word vectors included, for example:\n",
    "#✅ en_core_web_md (medium)\n",
    "#✅ en_core_web_lg (large)\n",
    "#🚫 NOT en_core_web_sm (small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "907826d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.869833325851152\n"
     ]
    }
   ],
   "source": [
    "# Load a larger pipeline with vectors\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Compare two documents\n",
    "doc1 = nlp(\"I like fast food\")\n",
    "doc2 = nlp(\"I like pizza\")\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b080d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6850197911262512\n"
     ]
    }
   ],
   "source": [
    "# Compare two tokens\n",
    "doc = nlp(\"I like pizza and pasta\")\n",
    "token1 = doc[2]\n",
    "token2 = doc[4]\n",
    "print(token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9bcd5a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18213694934365615\n"
     ]
    }
   ],
   "source": [
    "# Compare a document with a token\n",
    "doc = nlp(\"I like pizza\")\n",
    "token = nlp(\"soap\")[0]\n",
    "\n",
    "print(doc.similarity(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71a65d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47190033157126826\n"
     ]
    }
   ],
   "source": [
    "# Compare a span with a document\n",
    "span = nlp(\"I like pizza and pasta\")[2:5]\n",
    "doc = nlp(\"McDonalds sells burgers\")\n",
    "\n",
    "print(span.similarity(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3b1288",
   "metadata": {},
   "source": [
    "Similarity is determined using word vectors\n",
    "\n",
    "Multi-dimensional meaning representations of words\n",
    "\n",
    "Generated using an algorithm like Word2Vec and lots of text\n",
    "\n",
    "Can be added to spaCy's pipelines\n",
    "\n",
    "Default: cosine similarity, but can be adjusted\n",
    "\n",
    "Doc and Span vectors default to average of token vectors\n",
    "\n",
    "Short phrases are better than long documents with many irrelevant words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14528f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.20778  -2.4151    0.36605   2.0139   -0.23752  -3.1952   -0.2952\n",
      "  1.2272   -3.4129   -0.54969   0.32634  -1.0813    0.55626   1.5195\n",
      "  0.97797  -3.1816   -0.37207  -0.86093   2.1509   -4.0845    0.035405\n",
      "  3.5702   -0.79413  -1.7025   -1.6371   -3.198    -1.9387    0.91166\n",
      "  0.85409   1.8039   -1.103    -2.5274    1.6365   -0.82082   1.0278\n",
      " -1.705     1.5511   -0.95633  -1.4702   -1.865    -0.19324  -0.49123\n",
      "  2.2361    2.2119    3.6654    1.7943   -0.20601   1.5483   -1.3964\n",
      " -0.50819   2.1288   -2.332     1.3539   -2.1917    1.8923    0.28472\n",
      "  0.54285   1.2309    0.26027   1.9542    1.1739   -0.40348   3.2028\n",
      "  0.75381  -2.7179   -1.3587   -1.1965   -2.0923    2.2855   -0.3058\n",
      " -0.63174   0.70083   0.16899   1.2325    0.97006  -0.23356  -2.094\n",
      " -1.737     3.6075   -1.511    -0.9135    0.53878   0.49268   0.44751\n",
      "  0.6315    1.4963    4.1725    2.1961   -1.2409    0.4214    2.9678\n",
      "  1.841     3.0133   -4.4652    0.96521  -0.29787   4.3386   -1.2527\n",
      " -1.7734   -3.5637   -0.20035  -3.3013    0.99951  -0.92888  -0.94594\n",
      "  1.5124   -3.9385    2.7935   -3.1042    3.3382    0.54513  -0.37663\n",
      "  2.5151    0.51468  -0.88907   1.011     3.4705   -3.6037    1.3702\n",
      "  2.3468    1.6674    1.3904   -2.8112    2.237    -1.0344   -0.57164\n",
      "  1.0641   -1.6919    1.958    -0.78305   0.14741   0.51083   1.8278\n",
      " -0.69638   0.90548   0.62282  -1.8315   -2.8587    0.48424  -2.0527\n",
      " -0.53808  -2.3472    1.0354   -1.8257   -0.3892   -0.24943   0.8651\n",
      " -1.5195    1.2166   -2.698    -0.96698   2.2175   -0.16089  -0.49677\n",
      " -0.19646   1.3284    4.0824    1.3919    0.80669  -1.0316   -0.28056\n",
      " -1.8632    0.47716  -0.53628   1.3853   -2.1755   -0.2354    2.4933\n",
      " -0.87255   1.4493   -0.10778  -0.44159   1.3462    4.4211   -1.8385\n",
      "  0.3985    0.47637  -0.60074   3.3583   -0.15006  -0.40495   2.7225\n",
      " -1.6297    0.86797  -4.1445   -2.7793    1.1535   -0.011691  0.9792\n",
      " -1.0141    0.80134   0.43642   1.4337    2.8927    0.82871  -1.1827\n",
      " -1.3838    2.3903   -0.89323   1.1461   -1.7435    0.8654   -0.27075\n",
      " -0.78698   1.5631   -0.5923    0.098082 -0.26682   1.6282   -0.77495\n",
      "  3.2552    1.7964   -1.4314    1.2336    2.3102   -1.6328    2.8366\n",
      " -0.71384   0.43967   1.5627    3.079    -0.922    -0.43981  -0.7659\n",
      "  1.9362   -2.2479    1.041     0.63206   1.5855    3.4097   -2.9204\n",
      " -1.4751   -0.59534  -1.688    -4.1362    2.745    -2.8515    3.6509\n",
      " -0.66993  -2.8794    2.0733    1.1779   -2.0307    2.595    -0.12246\n",
      "  1.5844    1.1855    0.022385 -2.2916   -2.2684   -2.7537    0.34981\n",
      " -4.6243   -0.96521  -1.1435   -2.8894   -0.12619   2.9577   -1.7227\n",
      "  0.24757   1.2149    3.5349   -0.95802   0.080346 -1.6553   -0.6734\n",
      "  2.2918   -1.8229   -1.1336    1.8884    2.4789   -0.66061   2.0529\n",
      " -0.76687   0.32362  -2.2579    0.91278   0.36231   0.61562  -0.15396\n",
      " -0.42917  -0.89848   0.17298  -0.76978  -2.0222   -1.7127   -1.5632\n",
      "  0.56631  -1.354     2.6261    1.9156   -1.5651    1.8315   -1.4257\n",
      " -1.6861   -0.51953   1.7635   -0.50722   1.388    -1.1012  ]\n"
     ]
    }
   ],
   "source": [
    "# Load a larger pipeline with vectors\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "doc = nlp(\"I have a banana\")\n",
    "# Access the vector via the token.vector attribute\n",
    "print(doc[3].vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03757832",
   "metadata": {},
   "source": [
    "Useful for many applications: recommendation systems, flagging duplicates etc.\n",
    "    \n",
    "There's no objective definition of \"similarity\"\n",
    "\n",
    "Depends on the context and what application needs to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a92245f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9530093158841214\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"I like cats\")\n",
    "doc2 = nlp(\"I hate cats\")\n",
    "\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ab79c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.1689e-01 -2.5989e+00 -1.3144e+00  2.2500e+00 -4.6767e-01 -2.0695e+00\n",
      " -6.3379e-01 -4.0222e-01 -3.4022e+00 -3.6932e-01 -7.9938e-01 -1.0412e+00\n",
      "  9.3756e-01  1.6070e+00  8.8330e-01 -2.8483e+00  1.3349e-01 -3.1656e+00\n",
      "  8.1896e-01 -4.8113e+00  1.5655e+00  1.6665e+00 -4.7081e-01 -1.9475e+00\n",
      " -1.1779e+00 -1.3810e+00 -2.0071e+00 -2.1639e-01  9.0609e-01  1.5279e+00\n",
      "  1.2587e-04 -2.9000e+00  7.6069e-01 -2.2825e+00  1.2495e-02 -1.5653e+00\n",
      "  2.0052e+00 -1.7747e+00  5.9220e-01 -1.1428e+00 -1.3441e+00  3.4784e-01\n",
      "  1.7492e+00  1.9086e+00  1.0600e+00  1.2965e+00  4.1431e-01  7.9416e-01\n",
      " -1.1277e+00 -1.1403e+00  7.5891e-01 -9.4419e-01  1.4413e+00 -2.2554e+00\n",
      "  1.6226e-01  3.8901e-01  1.2299e-01  1.1577e+00  1.5524e+00  1.3853e+00\n",
      "  1.1112e+00  7.5767e-01  3.9431e+00 -2.8506e-01 -2.1645e+00 -1.0862e+00\n",
      " -1.4973e+00 -1.2781e+00  2.4643e+00 -1.5886e+00  2.5679e-01  6.4918e-01\n",
      "  1.6809e-01  5.7693e-01  3.1121e-01 -4.5278e-01 -2.7555e+00 -2.1846e+00\n",
      "  4.4865e+00  2.7107e-01 -5.3831e-01  8.3013e-01  6.7752e-02  1.4234e-01\n",
      "  1.2585e+00 -8.5423e-01  9.2971e-01 -3.9940e-01 -5.8663e-01  6.6604e-01\n",
      "  2.3871e+00  4.9333e-01  2.3922e+00 -3.7396e+00 -3.9524e-01 -6.3799e-01\n",
      "  3.3500e+00 -2.0430e+00 -1.5601e+00 -2.3594e+00 -1.4671e+00 -3.2848e+00\n",
      "  1.5197e+00 -1.1674e+00 -1.2885e+00  3.4890e+00 -4.0526e+00  1.6946e+00\n",
      " -1.5310e+00  2.6790e+00  1.2865e+00 -2.9286e-02  3.1037e+00  1.3635e-01\n",
      " -1.3327e-01  5.4603e-01  1.2937e+00 -2.3662e+00  2.8862e-01  1.6226e+00\n",
      "  6.3531e-01  1.5498e+00 -2.3349e+00  1.6150e+00 -2.0071e+00 -4.8784e-01\n",
      "  1.7768e+00 -2.6920e+00  1.6341e+00  4.0537e-01  3.7324e-01  7.8494e-01\n",
      "  1.3917e+00 -2.9035e-01  2.5224e+00  1.2485e+00 -1.5159e+00 -2.0832e+00\n",
      " -1.8766e-01 -1.3394e+00 -5.3597e-01 -2.4915e+00  1.6341e+00 -3.0336e+00\n",
      "  1.8791e-01 -2.4776e-01  1.6347e+00 -7.0009e-01  2.1221e+00 -2.3470e+00\n",
      " -2.1513e+00  1.2630e+00 -2.8195e-01 -5.6535e-01 -9.0373e-01  1.3455e+00\n",
      "  4.1099e+00  6.9325e-01  4.6835e-01  2.9949e-01 -3.2456e-01 -2.1713e+00\n",
      "  4.6691e-01 -1.3795e-01  6.0910e-01 -1.3374e+00 -8.3586e-01  1.8260e+00\n",
      " -4.5386e-01  1.2555e+00 -6.6705e-01 -3.0835e-01 -1.4692e-01  3.9952e+00\n",
      " -1.1289e+00  1.7926e-01 -5.8095e-01  6.2500e-01  2.6151e+00 -1.3212e+00\n",
      " -1.9355e+00  2.4898e+00 -1.7301e+00  8.6154e-01 -2.5272e+00 -3.0166e+00\n",
      "  5.8867e-01 -1.4207e-01  1.7611e+00 -1.0399e+00  5.7063e-01  7.2554e-01\n",
      "  8.7684e-01  1.6996e+00  1.1084e+00 -2.6877e-01 -2.2970e+00  1.8850e+00\n",
      "  1.2536e-01  1.1671e+00 -1.3038e+00  8.2640e-01  3.8002e-01 -1.0354e+00\n",
      "  2.2547e+00 -2.4526e-01  8.3622e-01 -7.9525e-01  1.4251e+00  4.5795e-02\n",
      "  2.5062e+00  7.2175e-01 -3.8197e-01  1.1387e+00  1.6779e+00 -1.6925e+00\n",
      "  8.2290e-02 -3.3117e-01 -6.1974e-01  8.0306e-01  2.1189e+00 -7.8231e-01\n",
      "  3.3266e-01 -7.1646e-01  1.7721e+00 -1.2573e+00  1.5601e-01  9.6129e-01\n",
      "  4.8986e-01  3.1657e+00 -3.0332e+00 -9.1339e-01 -2.8627e-01 -9.8174e-01\n",
      " -2.7599e+00  2.9160e+00 -1.1540e+00  2.9537e+00 -1.1502e+00 -2.2760e+00\n",
      "  1.1679e+00  2.0314e-01 -2.2882e+00  1.0485e+00 -4.9545e-01  2.4065e+00\n",
      "  9.1433e-01  1.3808e-01 -1.1361e+00 -1.2278e+00 -1.8958e+00 -2.4934e-01\n",
      " -4.1792e+00 -1.3781e+00 -8.9829e-01 -1.4802e+00 -1.2894e+00  2.2336e+00\n",
      " -2.4411e+00  3.0410e-01  2.1452e+00  1.8540e+00 -3.3640e-01  4.2768e-01\n",
      " -4.2577e-01  2.2337e-01  3.3650e+00 -2.1996e-01 -1.7426e+00  1.4554e+00\n",
      "  2.5087e+00 -1.1257e+00  1.4096e+00  1.3704e-01  7.6963e-01 -7.6688e-02\n",
      " -6.1875e-01 -2.0445e-01  4.9965e-01 -6.7246e-01 -4.0058e-01 -1.1536e+00\n",
      " -9.2837e-01 -2.5444e+00 -1.9880e-01 -2.1432e+00 -1.8218e+00 -5.7831e-01\n",
      " -1.6785e+00  2.6610e+00  1.0343e+00 -1.4297e+00  1.4563e+00 -2.0388e+00\n",
      " -1.4377e+00 -9.7985e-01  2.9488e+00 -2.7107e-01 -2.4010e-01 -1.3845e-01]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the en_core_web_md pipeline\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"Two bananas in pyjamas\")\n",
    "\n",
    "# Get the vector for the token \"bananas\"\n",
    "bananas_vector = doc[1].vector\n",
    "print(bananas_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02a51771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8220093114484404\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "doc1 = nlp(\"It's a warm summer day\")\n",
    "doc2 = nlp(\"It's sunny outside\")\n",
    "\n",
    "# Get the similarity of doc1 and doc2\n",
    "similarity = doc1.similarity(doc2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "33c19a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10219938308000565\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "doc = nlp(\"TV and books\")\n",
    "token1, token2 = doc[0], doc[2]\n",
    "\n",
    "# Get the similarity of the tokens \"TV\" and \"books\"\n",
    "similarity = token1.similarity(token2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b380caa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6348509788513184\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "doc = nlp(\"This was a great restaurant. Afterwards, we went to a really nice bar.\")\n",
    "\n",
    "# Create spans for \"great restaurant\" and \"really nice bar\"\n",
    "span1 = doc[3:5]\n",
    "span2 = doc[12:15]\n",
    "\n",
    "# Get the similarity of the spans\n",
    "similarity = span1.similarity(span2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6910a45d",
   "metadata": {},
   "source": [
    "Use cases\tapplication needs to generalize based on examples\t⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ ⠀⠀⠀⠀⠀⠀⠀\n",
    "Real-world examples\tproduct names, person names, subject/object relationships\t\n",
    "spaCy features\tentity recognizer, dependency parser, part-of-speech tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "672dd737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize with the shared vocab\n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Patterns are lists of dictionaries describing the tokens\n",
    "pattern = [{\"LEMMA\": \"love\", \"POS\": \"VERB\"}, {\"LOWER\": \"cats\"}]\n",
    "matcher.add(\"LOVE_CATS\", [pattern])\n",
    "\n",
    "# Operators can specify how often a token should be matched\n",
    "pattern = [{\"TEXT\": \"very\", \"OP\": \"+\"}, {\"TEXT\": \"happy\"}]\n",
    "matcher.add(\"VERY_HAPPY\", [pattern])\n",
    "\n",
    "# Calling matcher on doc returns list of (match_id, start, end) tuples\n",
    "doc = nlp(\"I love cats and I'm very very happy\")\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8eccbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched span: Golden Retriever\n",
      "Root token: Retriever\n",
      "Root head token: have\n",
      "Previous token: a DET\n"
     ]
    }
   ],
   "source": [
    "# adding statistical predictions\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"DOG\", [[{\"LOWER\": \"golden\"}, {\"LOWER\": \"retriever\"}]])\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print(\"Matched span:\", span.text)\n",
    "    # Get the span's root token and root head token\n",
    "    print(\"Root token:\", span.root.text)\n",
    "    print(\"Root head token:\", span.root.head.text)\n",
    "    # Get the previous token and its POS tag\n",
    "    print(\"Previous token:\", doc[start - 1].text, doc[start - 1].pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d80b30",
   "metadata": {},
   "source": [
    "PhraseMatcher like regular expressions or keyword search – but with access to the tokens!\n",
    "\n",
    "Takes Doc object as patterns\n",
    "\n",
    "More efficient and faster than the Matcher\n",
    "\n",
    "Great for matching large word lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9e47090d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched span: Golden Retriever\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "pattern = nlp(\"Golden Retriever\")\n",
    "matcher.add(\"DOG\", [pattern])\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Get the matched span\n",
    "    span = doc[start:end]\n",
    "    print(\"Matched span:\", span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df43f0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATTERN1 Amazon Prime\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN1 Amazon Prime\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN2 ad-free viewing\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\n",
    "    \"Twitch Prime, the perks program for Amazon Prime members offering free \"\n",
    "    \"loot, games and other benefits, is ditching one of its best features: \"\n",
    "    \"ad-free viewing. According to an email sent out to Amazon Prime members \"\n",
    "    \"today, ad-free viewing will no longer be included as a part of Twitch \"\n",
    "    \"Prime for new members, beginning on September 14. However, members with \"\n",
    "    \"existing annual subscriptions will be able to continue to enjoy ad-free \"\n",
    "    \"viewing until their subscription comes up for renewal. Those with \"\n",
    "    \"monthly subscriptions will have access to ad-free viewing until October 15.\"\n",
    ")\n",
    "\n",
    "# Create the match patterns\n",
    "pattern1 = [{\"LOWER\": \"amazon\"}, {\"IS_TITLE\": True, \"POS\": \"PROPN\"}]\n",
    "pattern2 = [{\"LOWER\": \"ad\"}, {\"TEXT\": \"-\"}, {\"LOWER\": \"free\"}, {\"POS\": \"NOUN\"}]\n",
    "\n",
    "# Initialize the Matcher and add the patterns\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"PATTERN1\", [pattern1])\n",
    "matcher.add(\"PATTERN2\", [pattern2])\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Print pattern string name and text of matched span\n",
    "    print(doc.vocab.strings[match_id], doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee5aed72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Czech Republic, Slovakia]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "with open(\"countries.json\", encoding=\"utf8\") as f:\n",
    "    COUNTRIES = json.loads(f.read())\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "\n",
    "# Import the PhraseMatcher and initialize it\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# Create pattern Doc objects and add them to the matcher\n",
    "# This is the faster version of: [nlp(country) for country in COUNTRIES]\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add(\"COUNTRY\", patterns)\n",
    "\n",
    "# Call the matcher on the test document and print the result\n",
    "matches = matcher(doc)\n",
    "print([doc[start:end] for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c544c3a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
