{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afbbde9a",
   "metadata": {},
   "source": [
    "This code generates poems using a Hidden Markov Model (HMM) based on Robert Frost's poems as training data. Let's break down what each part of the code does:\n",
    "\n",
    "Import Statements: The code begins by importing necessary libraries and modules, including numpy, string, sys, and functions from the future module to ensure compatibility with both Python 2 and Python 3.\n",
    "\n",
    "Data Structures Initialization: Several dictionaries are initialized to store information about the input text:\n",
    "\n",
    "initial: Stores the distribution of the first words of phrases.\n",
    "second_word: Stores the distribution of the second word given the first word.\n",
    "transitions: Stores the transitions between words in the text.\n",
    "Punctuation Removal: Depending on the Python version being used (2 or 3), the code selects one of two functions (remove_punctuation_2 or remove_punctuation_3) to remove punctuation from lines of text.\n",
    "\n",
    "Helper Function add2dict: This function adds values to a dictionary under a given key. If the key is not present, it creates a list for that key.\n",
    "\n",
    "Processing Input Text: The code reads lines from a file called 'robert_frost.txt', tokenizes them into words, converts the words to lowercase, and removes punctuation. It then processes these words to build the following statistics:\n",
    "\n",
    "initial: Measures the distribution of the first word in a phrase.\n",
    "second_word: Measures the distribution of the second word given the first word.\n",
    "transitions: Measures the transitions between words in the text, including the probability of ending a line ('END').\n",
    "Normalization: The distributions in initial are normalized by dividing the counts by the total count of initial words.\n",
    "\n",
    "Function list2pdict: This function takes a list of possibilities and converts it into a dictionary of probabilities.\n",
    "\n",
    "Converting Lists to Probability Distributions: The code converts lists of possibilities in second_word and transitions into dictionaries of probabilities using the list2pdict function.\n",
    "\n",
    "Function sample_word: This function samples a word from a probability distribution represented as a dictionary. It uses a random number to select a word based on its probability.\n",
    "\n",
    "Function generate: This function generates four lines of a poem. For each line, it does the following:\n",
    "\n",
    "Selects an initial word (w0) using the sample_word function.\n",
    "Selects a second word (w1) based on the distribution in second_word given w0.\n",
    "Continues to select words based on the transitions in transitions until an 'END' token is encountered. The selected words are added to the sentence.\n",
    "The generated poem is printed to the console.\n",
    "\n",
    "Finally, the generate function is called to generate four lines of a poem.\n",
    "The code aims to create poems in the style of Robert Frost's poetry by modeling the word sequences and transitions in his works. The generated poems are random but influenced by the statistics learned from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65d09053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with no small dignity of mien\n",
      "in eightyfive\n",
      "its too run down this is a good home i dont mean just skulls of rogers rangers\n",
      "and put my foot on one without avail\n"
     ]
    }
   ],
   "source": [
    "# https://deeplearningcourses.com/c/unsupervised-machine-learning-hidden-markov-models-in-python\n",
    "# https://udemy.com/unsupervised-machine-learning-hidden-markov-models-in-python\n",
    "# http://lazyprogrammer.me\n",
    "# Model and generate Robert Frost poems.\n",
    "from __future__ import print_function, division\n",
    "from future.utils import iteritems\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import string\n",
    "import sys\n",
    "\n",
    "\n",
    "initial = {} # start of a phrase\n",
    "second_word = {}\n",
    "transitions = {}\n",
    "\n",
    "# unfortunately these work different ways\n",
    "def remove_punctuation_2(s):\n",
    "    return s.translate(None, string.punctuation)\n",
    "\n",
    "def remove_punctuation_3(s):\n",
    "    return s.translate(str.maketrans('','',string.punctuation))\n",
    "\n",
    "if sys.version.startswith('2'):\n",
    "    remove_punctuation = remove_punctuation_2\n",
    "else:\n",
    "    remove_punctuation = remove_punctuation_3\n",
    "\n",
    "\n",
    "def add2dict(d, k, v):\n",
    "    if k not in d:\n",
    "        d[k] = []\n",
    "    d[k].append(v)\n",
    "\n",
    "for line in open('robert_frost.txt'):\n",
    "    tokens = remove_punctuation(line.rstrip().lower()).split()\n",
    "\n",
    "    T = len(tokens)\n",
    "    for i in range(T):\n",
    "        t = tokens[i]\n",
    "        if i == 0:\n",
    "            # measure the distribution of the first word\n",
    "            initial[t] = initial.get(t, 0.) + 1\n",
    "        else:\n",
    "            t_1 = tokens[i-1]\n",
    "            if i == T - 1:\n",
    "                # measure probability of ending the line\n",
    "                add2dict(transitions, (t_1, t), 'END')\n",
    "            if i == 1:\n",
    "                # measure distribution of second word\n",
    "                # given only first word\n",
    "                add2dict(second_word, t_1, t)\n",
    "            else:\n",
    "                t_2 = tokens[i-2]\n",
    "                add2dict(transitions, (t_2, t_1), t)\n",
    "\n",
    "\n",
    "# normalize the distributions\n",
    "initial_total = sum(initial.values())\n",
    "for t, c in iteritems(initial):\n",
    "    initial[t] = c / initial_total\n",
    "\n",
    "def list2pdict(ts):\n",
    "    # turn each list of possibilities into a dictionary of probabilities\n",
    "    d = {}\n",
    "    n = len(ts)\n",
    "    for t in ts:\n",
    "        d[t] = d.get(t, 0.) + 1\n",
    "    for t, c in iteritems(d):\n",
    "        d[t] = c / n\n",
    "    return d\n",
    "\n",
    "for t_1, ts in iteritems(second_word):\n",
    "    # replace list with dictionary of probabilities\n",
    "    second_word[t_1] = list2pdict(ts)\n",
    "\n",
    "for k, ts in iteritems(transitions):\n",
    "    transitions[k] = list2pdict(ts)\n",
    "\n",
    "# generate 4 lines\n",
    "def sample_word(d):\n",
    "    # print \"d:\", d\n",
    "    p0 = np.random.random()\n",
    "    # print \"p0:\", p0\n",
    "    cumulative = 0\n",
    "    for t, p in iteritems(d):\n",
    "        cumulative += p\n",
    "        if p0 < cumulative:\n",
    "            return t\n",
    "    assert(False) # should never get here\n",
    "\n",
    "def generate():\n",
    "    for i in range(4):\n",
    "        sentence =[]\n",
    "\n",
    "        # initial word\n",
    "        w0 = sample_word(initial)\n",
    "        sentence.append(w0)\n",
    "\n",
    "        # sample second word\n",
    "        w1 = sample_word(second_word[w0])\n",
    "        sentence.append(w1)\n",
    "\n",
    "        # second-order transitions until END\n",
    "        while True:\n",
    "            w2 = sample_word(transitions[(w0, w1)])\n",
    "            if w2 == 'END':\n",
    "                break\n",
    "            sentence.append(w2)\n",
    "            w0 = w1\n",
    "            w1 = w2\n",
    "        print(' '.join(sentence))\n",
    "\n",
    "generate()\n",
    "\n",
    "# exercise: make them rhyme!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1439f45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4ba2cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
