{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "595d12ba",
   "metadata": {},
   "source": [
    "Naive Bayes: Based on Bayes' theorem, Naive Bayes assumes all features are independent of each other.\n",
    "It calculates the probability of each class and the conditional probability of each class given a feature value,\n",
    "and then predicts the class with the highest probability.\n",
    "\n",
    "Logistic Regression: A statistical method that estimates the probability of a binary outcome based on one or more \n",
    "predictor variables. The output is transformed using the logistic function to ensure it lies between 0 and 1.\n",
    "\n",
    "SVM (Support Vector Machine): A linear classifier that aims to find the hyperplane that best separates the classes. \n",
    "For non-linear data, it uses the kernel trick to transform the data into a higher-dimensional space.\n",
    "\n",
    "Decision Tree: A flowchart-like structure where each internal node represents a feature test, each branch is an \n",
    "outcome of the test, and leaf nodes represent class labels. It recursively splits the data to maximize the separation \n",
    "of classes.\n",
    "\n",
    "Bagging: An ensemble method that trains multiple instances of a base algorithm on random subsets of the data.\n",
    "The final prediction is obtained by aggregating (e.g., majority vote for classification) the predictions of each instance.\n",
    "\n",
    "Random Forest: An ensemble of decision trees trained on random subsets of data and features. The final classification is\n",
    "determined by majority voting among the trees.\n",
    "\n",
    "Extra Trees: Like Random Forest, but it chooses random thresholds for splits on features rather than the optimal thresholds.\n",
    "    \n",
    "Ada Boosting: An iterative algorithm that adjusts the weights of training instances based on the errors of the previous\n",
    "classifier. New classifiers are trained to correct the mistakes of their predecessors.\n",
    "\n",
    "Gradient Boosting: Builds an additive model in a forward stage-wise fashion. New trees are fit to the negative gradient\n",
    "(residual errors) of the loss function.\n",
    "\n",
    "KNN (K-Nearest Neighbors): A non-parametric method that classifies a data point based on how its neighbors are classified.\n",
    "For a given data point, it finds the 'k' training examples that are closest to the point and returns the most \n",
    "common output value among them.\n",
    "\n",
    "Neural Networks: Inspired by the structure of the human brain, a neural network consists of interconnected layers of nodes \n",
    "(or \"neurons\"). Each connection has a weight, and each neuron applies an activation function to its input. During training,\n",
    "the network adjusts these weights to minimize the difference between its predictions and actual outputs. Deep learning \n",
    "involves using neural networks with many layers (deep neural networks) to model complex patterns and representations \n",
    "in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6b3bcde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn import neighbors,datasets,preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "fd1dbd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:, :2], iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=33)\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize an empty list to store results\n",
    "results = []\n",
    "\n",
    "for a in [GaussianNB(), SVC(), LogisticRegression(), DecisionTreeClassifier(), BaggingClassifier(), \n",
    "          RandomForestClassifier(), ExtraTreesClassifier(), AdaBoostClassifier(), \n",
    "          GradientBoostingClassifier(), neighbors.KNeighborsClassifier()]:\n",
    "    try:\n",
    "        \n",
    "        clf = a\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        # Append results to the list\n",
    "        results.append({\"Classifier\": type(a).__name__, \"Accuracy\": accuracy})\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with {type(a).__name__}: {e}\")\n",
    "\n",
    "# Convert results list to DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.sort_values('Accuracy',ascending=False).reset_index(drop=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
