{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73ccd95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start date: 2023-01-31 00:00:00\n",
      "end date: 2023-04-01 00:00:00\n",
      "Empty DataFrame\n",
      "Columns: [Article, Site]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xlwings as xw\n",
    "import os\n",
    "\n",
    "\n",
    "store_soh = pd.read_excel(\"store_soh.xlsx\")\n",
    "store_sales = pd.read_excel(\"store_sales.xlsx\")\n",
    "\n",
    "#Filter Three cols: Article, Site, Total Stock qauntity then aggregate them over qty to remove duplicates.\n",
    "store_soh_article_site = store_soh[['Article','Site','Total Stock Quantity']].groupby(['Article','Site'])['Total Stock Quantity'].sum().reset_index()\n",
    "\n",
    "## To change the number of days of sales\n",
    "\n",
    "# Set the filter condition\n",
    "n_days = 60\n",
    "\n",
    "#finding the last date in the store sales df\n",
    "max_date = store_sales['Day of Billing Date'].max()\n",
    "#filtering the last date n days before\n",
    "filter_date = max_date - pd.Timedelta(days=n_days)\n",
    "print('start date:',filter_date)\n",
    "print('end date:',max_date)\n",
    "# filtering  the df for those rows where data is greater than last date. \n",
    "filtered_store_sales = store_sales[store_sales['Day of Billing Date']> filter_date]\n",
    "#grouping store sales to eliminate any duplicate instance of Article and Qty and aggregate by POS Billing\n",
    "filtered_store_sales_sub = filtered_store_sales[['Article','Site','POS Billing Quantity','POS Gross Sales','POS Total Discount']].groupby(['Article','Site']).agg({'POS Billing Quantity':'sum','POS Gross Sales':'sum','POS Total Discount':'sum'}).reset_index()\n",
    "filtered_store_sales_sub\n",
    "\n",
    "# Now we want to make sure that all the combination of Article + Site are present in our consolidated data\n",
    "# To do that we take article + Site from both the store stock and sales data and concatenate\n",
    "# There will be overlaps between the two, for that we remove the duplicates\n",
    "\n",
    "# concatenatiing\n",
    "master_article_site = pd.concat([filtered_store_sales_sub[['Article','Site']],store_soh_article_site[['Article','Site']]])\n",
    "                                                           \n",
    "# removing duplicates\n",
    "master_article_site_dd = master_article_site.drop_duplicates(subset=['Article','Site'])\n",
    "master_article_site_dd                                                          \n",
    "\n",
    "\n",
    "Build_slst_df = master_article_site_dd.copy() #building sales stock df by left merging both the dataframes\n",
    "# But first we need to see if all the articles are present in the MCC sheet\n",
    "\n",
    "mcc = pd.read_excel(\"mcc.xlsx\")\n",
    "\n",
    "# dropping  articles which are duplicates as we need to vlookup the attributes\n",
    "mcc = mcc.drop_duplicates(subset='Article')\n",
    "\n",
    "mcc.columns\n",
    "\n",
    "master = mcc.copy()\n",
    "master.loc[:,'Brand'] = mcc['Brand'].str.upper().copy()\n",
    "\n",
    "#Checking if all the Articles in the sales stock are contained in the master file\n",
    "if Build_slst_df['Article'].isin(master['Article']).all():\n",
    "    a = \"yes\"\n",
    "else:\n",
    "    a = \"No\"\n",
    "\n",
    "# Getting the rows in the master sheet for which the articles are not there in the Build...df\n",
    "not_contained = ~Build_slst_df['Article'].isin(master['Article'])\n",
    "rows_not_contained = Build_slst_df[not_contained]\n",
    "print(rows_not_contained)\n",
    "\n",
    "Build_slst_df.duplicated().sum()\n",
    "\n",
    "ss_df_mcc = Build_slst_df.merge(master.drop_duplicates(subset='Article'),how='inner',on='Article')\n",
    "ss_df_mcc\n",
    "\n",
    "#Now vlooking the sales quantitites from sales data\n",
    "ss_df_mcc_stsoh = ss_df_mcc.merge(filtered_store_sales_sub,how='left',on=['Article','Site'])\n",
    "ss_df_mcc_stsoh\n",
    "\n",
    "# vlooking the stock quantities from stock data\n",
    "ss = ss_df_mcc_stsoh.merge(store_soh_article_site,how='left',on=['Article','Site']).fillna(0)\n",
    "ss\n",
    "\n",
    "ss.columns\n",
    "\n",
    "ss.groupby([' C Technique','MRP Bin']).agg({'POS Billing Quantity':'sum','Total Stock Quantity':'sum',\n",
    "                                           'POS Gross Sales':'sum','POS Total Discount':'sum'}).reset_index()\n",
    "\n",
    "\n",
    "# this is a function to create a dataframe with all sites and all crafts\n",
    "def salesstock_all(metric):\n",
    "    consolidated_df = pd.DataFrame()\n",
    "\n",
    "    for brand in ss['Brand'].unique():\n",
    "        for site in ss['Site'].unique():\n",
    "\n",
    "            #filtering the brand and the site from the final sales to stock data\n",
    "            ss_site_brand = ss[(ss['Brand']==brand) & (ss['Site']==site)]\n",
    "\n",
    "            #Aggregating the sales and stock dataa as per the metric ( eg. C-Tehnique)\n",
    "            ss_final = ss_site_brand.groupby([' C Technique','MRP Bin']).agg({'POS Billing Quantity':'sum','Total Stock Quantity':'sum',\n",
    "            'POS Gross Sales':'sum','POS Total Discount':'sum'}).reset_index()\n",
    "\n",
    "            # Creating sales and stock contribution columns respectively\n",
    "            ss_final[\"%sales_cont\"] = (ss_final['POS Billing Quantity']*100/ss_final['POS Billing Quantity'].sum().round(10)).fillna(0)\n",
    "\n",
    "            ss_final[\"%stock_cont\"] = (ss_final['Total Stock Quantity']*100/ss_final['Total Stock Quantity'].sum().round(10)).fillna(0)\n",
    "            ss_final[\"Sales/SOH\"] = ((ss_final['%sales_cont'])/(ss_final['%stock_cont'])).fillna(0)\n",
    "            ss_final[\"MD%\"] = ((ss_final['POS Total Discount'])/(ss_final[\"POS Gross Sales\"])).fillna(999)\n",
    "            ss_final['rank_sales']=ss_final['Sales/SOH'].rank(ascending=False,method='min',na_option='bottom')\n",
    "            ss_final['rank_md'] = ss_final['MD%'].rank(ascending=False,method='min',na_option='bottom')\n",
    "            max_rank = ss_final['rank_md'].max()\n",
    "            ss_final['rev_rank_md'] = max_rank - ss_final['rank_md']+1\n",
    "            #set rank to the Lowest if the corresponding value in another column is zero\n",
    "\n",
    "\n",
    "            #sorting the values by sales contribution\n",
    "            ss_final = ss_final.sort_values(by='%sales_cont',ascending=False)\n",
    "\n",
    "            # Add columns for brand and sites\n",
    "            ss_final['Brand'] = brand\n",
    "            ss_final['Site'] = site\n",
    "            ss_final['resultant'] = (0.70*ss_final['rank_sales'])+(0.30*ss_final['rev_rank_md'])\n",
    "\n",
    "\n",
    "\n",
    "            # create a mask to identify rows where the condition is not met\n",
    "            mask = ss_final['MD%'] != 999\n",
    "\n",
    "            # Assign ranks to the rows where the condition is met, and NaN to the rows where the condition is not met\n",
    "            ss_final['frank'] = ss_final.loc[mask,'resultant'].rank(ascending=True,method='min',na_option='bottom')-1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # Append the current DataFrame to the consolidated DataFrame\n",
    "            consolidated_df = pd.concat([consolidated_df,ss_final[['Brand','Site',metric,'MRP Bin','POS Billing Quantity','Total Stock Quantity','POS Gross Sales','POS Total Discount',\n",
    "            '%sales_cont','%stock_cont','Sales/SOH','MD%','rank_sales','rank_md','rev_rank_md','resultant','frank']]])\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    return consolidated_df\n",
    "\n",
    "\n",
    "# this is a function to generate a dataframe with all crafts and all sites.\n",
    "ssa = salesstock_all(' C Technique')\n",
    "ssa\n",
    "\n",
    "average_temp = ssa[ssa['POS Billing Quantity']>0]\n",
    "average_temp\n",
    "\n",
    "# Calculate the average,maximum, and minimum values for POS Billing Quantity for each Site Name\n",
    "grouped = average_temp.groupby(['Brand','Site'])['POS Billing Quantity'].agg(['mean','max','min'])\n",
    "\n",
    "#Rename the columns for clarity\n",
    "grouped.columns = ['Average Sales','Maximum Sales','Minimum Sales']\n",
    "\n",
    "# Merge the grouped dataframe with the original dataframe based on 'Site Name'\n",
    "average_temp = average_temp.merge(grouped,on=['Brand','Site'],how = 'left')\n",
    "\n",
    "average_temp\n",
    "\n",
    "# Create a new column 'num_rows' counting the number of rows for each Brand on Site Name\n",
    "average_temp['num_rows']= average_temp.groupby(['Brand','Site'])['Brand'].transform('size')\n",
    "average_temp\n",
    "\n",
    "average_temp['adjust'] = average_temp['Average Sales'] + (average_temp['num_rows']-average_temp['frank'])+((average_temp['Maximum Sales']-average_temp['Minimum Sales'])/average_temp['num_rows'])\n",
    "\n",
    "# Calculate the sum of 'adjust' for each site name\n",
    "site_adjust_sum = average_temp.groupby(['Brand','Site'])['adjust'].transform('sum')\n",
    "\n",
    "# Calculate the % contribution of 'adjust' for each row\n",
    "average_temp['adjust_contribution'] = ((average_temp['adjust']/site_adjust_sum)*100)\n",
    "average_temp\n",
    "\n",
    "#average_temp['gap%'] = average_temp['adjust_contribution']-average_temp[\"%stock_cont\"] if average_temp['adjust_contribution']<100 else 100\n",
    "#average_temp\n",
    "\n",
    "#import pandas as pd\n",
    "\n",
    "# Assuming you have a DataFrame called average_temp with columns 'adjust_contribution' and '%stock_cont'\n",
    "\n",
    "average_temp['gap%'] = average_temp.apply(lambda row: row['adjust_contribution'] - row['%stock_cont'] \n",
    "                                          if row['adjust_contribution'] < 100 else 100, axis=1)\n",
    "\n",
    "\n",
    "average_temp\n",
    "\n",
    "discap = pd.read_excel(\"discap.xlsx\")\n",
    "discap\n",
    "\n",
    "discap = discap[discap['Display Capacity']>0]\n",
    "discap\n",
    "\n",
    "# Step 1 : Merge the DataFrames based on Brand and Site Name using an inner merge\n",
    "merged_df = average_temp.merge(discap,on=[\"Brand\",\"Site\"],how=\"inner\")\n",
    "\n",
    "# Step 2: Assign the Display Capacity values to the corresponding rows in df1\n",
    "average_temp['Display Capacity'] = merged_df['Display Capacity']\n",
    "\n",
    "average_temp['gap%'] = average_temp['gap%'].apply(lambda x:0 if x<0 else x)\n",
    "average_temp\n",
    "\n",
    "# Step 1 : Merge the DataFrames based on Brand and Site Name using an inner merge\n",
    "groupbydf = average_temp.groupby([\"Brand\",\"Site\"])['Total Stock Quantity'].sum().reset_index()\n",
    "groupbydf\n",
    "\n",
    "# Step 1 : Merge the DataFrames based on Brand and Site Name using an inner merge\n",
    "merged_df = average_temp.merge(groupbydf,on=[\"Brand\",\"Site\"],how=\"inner\")\n",
    "merged_df\n",
    "\n",
    "# Step 2: Assign the Display Capacity values to the corresponding rows in df1\n",
    "average_temp['BrandStock'] = merged_df['Total Stock Quantity_y']\n",
    "average_temp['Balance'] = average_temp['Display Capacity']-average_temp['BrandStock']\n",
    "\n",
    "# Calculate the total gap for each group (brand and site)\n",
    "average_temp['total_gap'] = average_temp.groupby(['Brand', 'Site'])['gap%'].transform('sum')\n",
    "\n",
    "# Calculate the normalized gap (normgap)\n",
    "average_temp['normgap'] = average_temp['gap%']/ average_temp['total_gap']\n",
    "\n",
    "average_temp[\"final_gap\"] = (average_temp['normgap']*average_temp['Balance'])\n",
    "average_temp[\"final_gap\"] = average_temp[\"final_gap\"].fillna(0).astype(int)\n",
    "average_temp\n",
    "\n",
    "xw.view(average_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3091950c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
