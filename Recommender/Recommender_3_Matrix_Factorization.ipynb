{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2795372a",
   "metadata": {},
   "source": [
    "### Intuition Behind Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e08fda",
   "metadata": {},
   "source": [
    "Matrix factorization is a fundamental technique used in recommender systems. It's a bit complex, but I'll try to explain it in an intuitive way:\n",
    "\n",
    "**Understanding the Matrix**: Imagine you have a large matrix (a grid of numbers) where one dimension represents users and the other represents items (like movies or products). Each cell in this matrix contains the rating a user has given to an item. However, not all cells are filled because not every user has rated every item.\n",
    "\n",
    "**The Problem of Missing Data**: The main challenge in building a recommender system is that this matrix is mostly empty â€” most users have only rated a few items. The goal is to predict the missing ratings so we can recommend items that users are likely to enjoy.\n",
    "\n",
    "**Factorization Concept**: Matrix factorization breaks down this large, mostly empty matrix into two smaller, more manageable matrices. These smaller matrices are usually called 'user factors' and 'item factors'.\n",
    "\n",
    "**User Factors and Item Factor**s:\n",
    "\n",
    "**User Factors**: This matrix represents each user with a set of factors or features, which are inferred from their ratings.\n",
    "**Item Factors**: Similarly, this matrix represents each item with a set of factors.\n",
    "**Discovering Latent Features**: The factors here are 'latent features'. They might represent hidden characteristics of items (like genre, style, etc. for movies) and user preferences that align with these characteristics. These are not explicitly stated but are inferred from the ratings.\n",
    "\n",
    "**Multiplying to Predict Ratings**: When we multiply these two smaller matrices, we can approximate the original large matrix. The key idea is that the product of user factors and item factors gives us the predicted ratings for all the items, including those not yet rated by a user.\n",
    "\n",
    "**Learning the Factors**: The system learns these factors by minimizing the difference between the known ratings and the ratings predicted by the multiplication of the user and item factors. This process usually involves optimization techniques like gradient descent.\n",
    "\n",
    "**Personalized Recommendation**s: Once the system has learned these factors accurately, it can predict what ratings a user might give to an unwatched movie or unrated product. These predictions are used to recommend items to the user.\n",
    "\n",
    "In summary, matrix factorization for recommender systems is about breaking down a large, sparse matrix of user-item interactions into smaller, denser matrices that capture the underlying preferences and characteristics. By doing so, the system can predict unknown ratings and make personalized recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4148ea31",
   "metadata": {},
   "source": [
    "### Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fea0dcd",
   "metadata": {},
   "source": [
    " Let's create a simple example of matrix factorization for a recommender system in Python. We'll use a small dataset for ease of understanding. Our dataset will be a small user-item rating matrix, and we'll apply matrix factorization to predict missing ratings.\n",
    "\n",
    "First, we need to set up a basic environment for this task:\n",
    "\n",
    "We'll create a small matrix of user ratings for different items (movies, for instance).\n",
    "We'll use NumPy for matrix operations, and a simple matrix factorization method.\n",
    "In this example, let's assume we have 5 users and 4 items (movies). The users have rated some movies, but not all. Our task is to predict the missing ratings.\n",
    "\n",
    "\n",
    "The Python code above performed matrix factorization on a simple user-item ratings matrix. Here's a breakdown of what we did:\n",
    "\n",
    "Created a Ratings Matrix (R): This matrix represented the ratings given by 5 users to 4 items. Ratings ranged from 1 to 5, and a rating of 0 indicated a missing rating.\n",
    "\n",
    "Initialized User and Item Feature Matrices (U and I): These matrices represent the latent features of users and items. We initialized them with random values.\n",
    "\n",
    "Set the Parameters: We chose the number of latent features (K), the number of iterations for the optimization process, the learning rate, and the regularization parameter.\n",
    "\n",
    "Matrix Factorization Process:\n",
    "\n",
    "We used a method called Stochastic Gradient Descent (SGD) to find the optimal values for U and I.\n",
    "In each iteration, we updated the values in U and I based on the gradient of the error between the actual rating and the predicted rating.\n",
    "We only considered the non-zero ratings (i.e., the known ratings) during the update process.\n",
    "Predicted Ratings: After the matrix factorization, we used the final user and item feature matrices to predict the full ratings matrix. This matrix provides estimates for the missing ratings.\n",
    "\n",
    "The resulting predicted_ratings matrix shows the estimated ratings for all user-item pairs, including those that were originally missing in the R matrix. These predictions can be used to recommend items to users based on their predicted interests.\n",
    "\n",
    "This example is a basic demonstration and serves to illustrate the concept. In real-world scenarios, you would use more sophisticated techniques and larger datasets, potentially with additional optimizations and enhancements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b4d2a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.97875256, 2.98180954, 3.45923688, 0.99917092],\n",
       "       [3.9775769 , 2.39895867, 2.99182212, 1.00468272],\n",
       "       [1.00407223, 0.98865594, 5.97113337, 4.97199009],\n",
       "       [0.99809522, 0.90487661, 4.87500487, 3.98297405],\n",
       "       [1.17979621, 1.01198091, 4.97786318, 3.9982824 ]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample ratings matrix (users x items)\n",
    "# Ratings range from 1 to 5; 0 indicates a missing rating\n",
    "R = np.array([\n",
    "    [5, 3, 0, 1],\n",
    "    [4, 0, 3, 1],\n",
    "    [1, 1, 0, 5],\n",
    "    [1, 0, 0, 4],\n",
    "    [0, 1, 5, 4],\n",
    "])\n",
    "\n",
    "# Number of factors (latent features)\n",
    "K = 2\n",
    "\n",
    "# Initialize user and item latent feature matrices\n",
    "# Random values are used for the initialization\n",
    "np.random.seed(0)  # for reproducibility\n",
    "U = np.random.rand(R.shape[0], K)  # User features matrix\n",
    "I = np.random.rand(R.shape[1], K)  # Item features matrix\n",
    "\n",
    "# Parameters for matrix factorization\n",
    "iterations = 5000  # Number of iterations for the optimization\n",
    "learning_rate = 0.01  # Learning rate\n",
    "regularization = 0.02  # Regularization parameter to avoid overfitting\n",
    "\n",
    "# Function to perform matrix factorization\n",
    "def matrix_factorization(R, U, I, K, iterations, learning_rate, regularization):\n",
    "    # Perform Stochastic Gradient Descent\n",
    "    for iteration in range(iterations):\n",
    "        for i in range(len(R)):\n",
    "            for j in range(len(R[i])):\n",
    "                if R[i][j] > 0:  # only consider non-zero ratings\n",
    "                    # Calculate error\n",
    "                    eij = R[i][j] - np.dot(U[i, :], I[j, :].T)\n",
    "                    # Update user and item latent feature matrices\n",
    "                    U[i, :] += learning_rate * (2 * eij * I[j, :] - regularization * U[i, :])\n",
    "                    I[j, :] += learning_rate * (2 * eij * U[i, :] - regularization * I[j, :])\n",
    "    \n",
    "    return U, I\n",
    "\n",
    "# Perform matrix factorization\n",
    "U, I = matrix_factorization(R, U, I, K, iterations, learning_rate, regularization)\n",
    "\n",
    "# Predict the full ratings matrix\n",
    "predicted_ratings = np.dot(U, I.T)\n",
    "predicted_ratings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9760ba",
   "metadata": {},
   "source": [
    "### Vectorized Code ( Same Output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe2648e",
   "metadata": {},
   "source": [
    "Vectorizing the code for matrix factorization can make it more efficient, especially for larger datasets. Vectorized operations in Python, typically using NumPy, are generally faster than explicit loops over array elements.\n",
    "\n",
    "Let's refactor the matrix factorization process using vectorized operations. The core idea remains the same, but we'll replace the explicit loops with matrix operations where possible.\n",
    "\n",
    "The vectorized version of the matrix factorization code performs the same task as the previous version, but with more efficient operations. Here's a summary of the changes made:\n",
    "\n",
    "**Vectorized User and Item Updates**: Instead of iterating over each user-item pair individually, the code now processes all the ratings of a single user or a single item in one go. This is done using matrix operations which are inherently faster in NumPy.\n",
    "\n",
    "**Efficient Error Calculation**: For each user and item, the error between the actual ratings and the predicted ratings is computed in a vectorized manner. This significantly reduces the computational overhead compared to individual error calculations.\n",
    "\n",
    "**Updated Feature Matrices**:\n",
    "\n",
    "For each user, the user feature vector is updated based on the error across all items rated by that user.\n",
    "Similarly, for each item, the item feature vector is updated based on the error across all users who have rated that item.\n",
    "Final Predicted Ratings: The final predicted ratings are calculated by multiplying the updated user and item feature matrices, just like in the non-vectorized version.\n",
    "\n",
    "The output predicted_ratings_vectorized shows the estimated ratings for all user-item pairs, including the previously missing ratings. The values might differ slightly from the non-vectorized version due to the nature of floating-point calculations in Python, but the overall concept and results are consistent.\n",
    "\n",
    "This vectorized approach is more suitable for larger datasets, as it significantly reduces computation time compared to the loop-based approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6562bc2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.98861364, 2.98660352, 3.4615647 , 0.99869167],\n",
       "       [3.98779274, 2.4040358 , 2.99352822, 1.00437938],\n",
       "       [1.00519089, 0.98708862, 5.95052627, 4.98145458],\n",
       "       [0.99737953, 0.90217293, 4.85114748, 3.98439629],\n",
       "       [1.18911693, 1.01614642, 4.97305894, 4.01264932]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def matrix_factorization_vectorized(R, U, I, K, iterations, learning_rate, regularization):\n",
    "    users, items = R.shape\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        for i in range(users):\n",
    "            # Indices of items rated by user i\n",
    "            idx = np.where(R[i, :] > 0)[0]\n",
    "            I_rated = I[idx, :]\n",
    "            \n",
    "            # Error for the items rated by user i\n",
    "            E_i = R[i, idx] - np.dot(U[i, :], I_rated.T)\n",
    "\n",
    "            # Update rule for user features\n",
    "            U[i, :] += learning_rate * (np.dot(E_i, I_rated) - regularization * U[i, :])\n",
    "\n",
    "        for j in range(items):\n",
    "            # Indices of users who rated item j\n",
    "            idx = np.where(R[:, j] > 0)[0]\n",
    "            U_rated = U[idx, :]\n",
    "            \n",
    "            # Error for the users who rated item j\n",
    "            E_j = R[idx, j] - np.dot(U_rated, I[j, :])\n",
    "\n",
    "            # Update rule for item features\n",
    "            I[j, :] += learning_rate * (np.dot(E_j.T, U_rated) - regularization * I[j, :])\n",
    "    \n",
    "    return U, I\n",
    "\n",
    "# Re-initialize user and item latent feature matrices\n",
    "np.random.seed(0)\n",
    "U_v = np.random.rand(R.shape[0], K)  # User features matrix\n",
    "I_v = np.random.rand(R.shape[1], K)  # Item features matrix\n",
    "\n",
    "# Perform matrix factorization with vectorized operations\n",
    "U_v, I_v = matrix_factorization_vectorized(R, U_v, I_v, K, iterations, learning_rate, regularization)\n",
    "\n",
    "# Predict the full ratings matrix with vectorized matrices\n",
    "predicted_ratings_vectorized = np.dot(U_v, I_v.T)\n",
    "predicted_ratings_vectorized\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c561bdbd",
   "metadata": {},
   "source": [
    "### Code by Splitting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b530bac",
   "metadata": {},
   "source": [
    "splitting the data into training and test sets is a common practice in machine learning, including in building recommender systems. This allows us to train the model on a subset of the data (training set) and then evaluate its performance on a separate subset (test set) that the model hasn't seen during training. This approach gives a better estimate of how well the model will perform on new, unseen data.\n",
    "\n",
    "For this example, let's:\n",
    "\n",
    "Split our existing ratings matrix into a training set and a test set.\n",
    "Train the matrix factorization model on the training set.\n",
    "Evaluate the model's performance on the test set.\n",
    "We'll randomly select a percentage of the known ratings to be part of the test set while ensuring that every user and item has at least one rating in the training set. This step is crucial to avoid cold start issues where a user or item has no ratings in the training set. Let's write the code for this process.\n",
    "\n",
    "The code has successfully split the original ratings matrix into training and test sets, trained the matrix factorization model on the training data, and made predictions for both sets. Here's an overview:\n",
    "\n",
    "**Training and Test Sets Creation**:\n",
    "\n",
    "The create_train_test_sets function converts the ratings matrix into a list of (user, item, rating) tuples.\n",
    "It then splits this list into training and test sets, ensuring that every user and item is present in the training set.\n",
    "The training and test sets are converted back into matrix form, with zeros in places where ratings are missing.\n",
    "Matrix Factorization on Training Data:\n",
    "\n",
    "We reinitialized the user and item feature matrices (U_train and I_train) and performed matrix factorization on the training set using the vectorized function.\n",
    "**Predictions**:\n",
    "\n",
    "Predicted ratings for the training set (predicted_train_ratings) were generated by multiplying the learned user and item feature matrices.\n",
    "Predictions for the test set (predicted_test_ratings) were obtained similarly, but we only kept the predictions for the entries that are actually part of the test set (i.e., where test_R is non-zero).\n",
    "\n",
    "**Output Matrices**:\n",
    "\n",
    "train_R is the training ratings matrix with known ratings used for training and zeros elsewhere.\n",
    "test_R is the test ratings matrix with known ratings used for testing and zeros elsewhere.\n",
    "predicted_train_ratings shows the predicted ratings for the training data.\n",
    "predicted_test_ratings shows the predicted ratings for the test data, with zeros for non-test entries.\n",
    "This process allows us to evaluate the performance of our matrix factorization model by comparing the predicted ratings against the actual ratings in the test set. In a real-world application, you would use metrics like Root Mean Squared Error (RMSE) to quantify the model's prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b719314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[5., 3., 0., 1.],\n",
       "        [4., 0., 0., 1.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [1., 0., 0., 4.],\n",
       "        [0., 1., 5., 4.]]),\n",
       " array([[0., 0., 0., 0.],\n",
       "        [0., 0., 3., 0.],\n",
       "        [1., 0., 0., 5.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]]),\n",
       " array([[ 4.98968052,  2.98732565,  3.28587718,  0.99813684],\n",
       "        [ 3.98545094,  2.3387148 ,  2.80315011,  0.99976286],\n",
       "        [ 1.69155648,  0.99065695,  1.19717718,  0.43275517],\n",
       "        [ 1.00059267, -0.28508251,  3.99172481,  3.97933104],\n",
       "        [ 3.00712893,  1.00491708,  4.97879699,  4.00164365]]),\n",
       " array([[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  2.80315011,  0.        ],\n",
       "        [ 1.69155648,  0.        ,  0.        ,  0.43275517],\n",
       "        [ 0.        , -0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_train_test_sets(R, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Create training and test sets from the ratings matrix.\n",
    "    Ensures every user and item has at least one rating in the training set.\n",
    "    \"\"\"\n",
    "    # Convert the ratings matrix to a list of user-item-rating tuples\n",
    "    ratings_list = [(i, j, R[i, j]) for i in range(R.shape[0]) for j in range(R.shape[1]) if R[i, j] > 0]\n",
    "\n",
    "    # Train-test split\n",
    "    train_data, test_data = train_test_split(ratings_list, test_size=test_size, random_state=0)\n",
    "\n",
    "    # Convert lists back to matrices\n",
    "    train_matrix = np.zeros(R.shape)\n",
    "    test_matrix = np.zeros(R.shape)\n",
    "    for i, j, rating in train_data:\n",
    "        train_matrix[i, j] = rating\n",
    "    for i, j, rating in test_data:\n",
    "        test_matrix[i, j] = rating\n",
    "\n",
    "    return train_matrix, test_matrix\n",
    "\n",
    "# Create training and test sets\n",
    "train_R, test_R = create_train_test_sets(R)\n",
    "\n",
    "# Re-initialize user and item latent feature matrices for training\n",
    "U_train = np.random.rand(R.shape[0], K)\n",
    "I_train = np.random.rand(R.shape[1], K)\n",
    "\n",
    "# Perform matrix factorization on the training set\n",
    "U_train, I_train = matrix_factorization_vectorized(train_R, U_train, I_train, K, iterations, learning_rate, regularization)\n",
    "\n",
    "# Predict ratings on the training set\n",
    "predicted_train_ratings = np.dot(U_train, I_train.T)\n",
    "\n",
    "# Predict ratings on the test set\n",
    "predicted_test_ratings = np.dot(U_train, I_train.T) * (test_R > 0)  # Zero out predictions for non-test entries\n",
    "\n",
    "# Return the train and test matrices for inspection\n",
    "train_R, test_R, predicted_train_ratings, predicted_test_ratings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b08e1e8",
   "metadata": {},
   "source": [
    "### Using Keras and Embedding Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277489b0",
   "metadata": {},
   "source": [
    "To implement matrix factorization using Keras with embedded layers, we essentially create an embedding for users and an embedding for items (movies, products, etc.). The key idea is to represent users and items in a shared latent space and then to predict ratings by computing the dot product of these embeddings.\n",
    "\n",
    "Here's a step-by-step guide on how to do this:\n",
    "\n",
    "**Prepare the Data**: Ensure that user IDs and item IDs are indexed properly (e.g., 0 to N-1 for users and 0 to M-1 for items, where N is the number of users and M is the number of items).\n",
    "\n",
    "**Create Embeddings**:\n",
    "\n",
    "**User Embedding**: This layer learns an embedding for each user.\n",
    "**Item Embedding**: This layer learns an embedding for each item.\n",
    "**Dot Product**: After obtaining the embeddings, compute the dot product between the user and item embeddings to get the predicted rating.\n",
    "\n",
    "**Model Definition**: Define a Keras model that takes a user and an item as input and outputs a predicted rating.\n",
    "\n",
    "**Compile and Train**: Compile the model, specifying an appropriate loss function (like mean squared error for regression) and an optimizer. Then, train the model on the training data.\n",
    "\n",
    "**Evaluation**: Evaluate the model on test data.\n",
    "\n",
    "Let's write the code for this. We'll assume that user IDs and item IDs are already appropriately indexed. We'll use the same dataset (matrix R) as before for simplicity, and we'll keep the same train-test split. We need to adjust the data format to be compatible with Keras, and then we can create and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9278edb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 10.1839\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 10.1828\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 10.1817\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 10.1806\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 10.1794\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 10.1783\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 10.1772\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 10.1760\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 10.1748\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 10.1737\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 10.1725\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 10.1713\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 10.1701\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 10.1689\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 10.1676\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 10.1664\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 10.1651\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 10.1638\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 10.1624\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 10.1611\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 10.1596\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 10.1582\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 10.1567\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 10.1552\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 10.1536\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 10.1519\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 10.1503\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 10.1485\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 10.1467\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 10.1448\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 10.1429\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 10.1409\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 10.1388\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 10.1366\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 10.1344\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 10.1321\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 10.1297\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 10.1272\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 10.1247\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 10.1220\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 10.1193\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 10.1164\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 10.1135\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 10.1104\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 10.1073\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 10.1041\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 10.1007\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 10.0973\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 10.0937\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 10.0901\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 10.0863\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 10.0824\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 10.0784\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 10.0742\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 10.0700\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 10.0656\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 10.0611\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 10.0565\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 10.0517\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 10.0469\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 10.0418\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 10.0367\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 10.0314\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 10.0261\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 10.0205\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 10.0149\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 10.0091\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 10.0031\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 9.9971\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 9.9909\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 9.9845\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 9.9780\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 9.9714\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 9.9647\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 9.9578\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.9508\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 9.9436\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 9.9363\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 9.9288\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 9.9212\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 9.9135\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 9.9056\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 9.8976\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.8895\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.8812\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 9.8727\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 9.8642\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 9.8555\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 9.8466\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 9.8376\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 9.8285\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 9.8192\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 9.8098\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 9.8002\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.7905\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 9.7807\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 9.7707\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 9.7606\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 9.7504\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 9.7400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1a3103d4b10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting user, item indices, and ratings from the train_R matrix for training the Keras model\n",
    "train_user_indices = []\n",
    "train_item_indices = []\n",
    "train_ratings = []\n",
    "\n",
    "for user_id in range(train_R.shape[0]):\n",
    "    for item_id in range(train_R.shape[1]):\n",
    "        rating = train_R[user_id, item_id]\n",
    "        if rating > 0:  # If rating is present\n",
    "            train_user_indices.append(user_id)\n",
    "            train_item_indices.append(item_id)\n",
    "            train_ratings.append(rating)\n",
    "\n",
    "# Convert lists to numpy arrays for Keras\n",
    "train_user_indices = np.array(train_user_indices)\n",
    "train_item_indices = np.array(train_item_indices)\n",
    "train_ratings = np.array(train_ratings)\n",
    "\n",
    "# Training the model (assuming the model mf_model is already built)\n",
    "mf_model.fit([train_user_indices, train_item_indices], train_ratings, epochs=100, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f519d67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 207ms/step\n",
      "Test RMSE: 3.36344431403127\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Extracting test data\n",
    "test_user_indices = []\n",
    "test_item_indices = []\n",
    "test_actual_ratings = []\n",
    "\n",
    "for user_id in range(test_R.shape[0]):\n",
    "    for item_id in range(test_R.shape[1]):\n",
    "        rating = test_R[user_id, item_id]\n",
    "        if rating > 0:  # If rating is present\n",
    "            test_user_indices.append(user_id)\n",
    "            test_item_indices.append(item_id)\n",
    "            test_actual_ratings.append(rating)\n",
    "\n",
    "# Convert lists to numpy arrays for prediction\n",
    "test_user_indices = np.array(test_user_indices)\n",
    "test_item_indices = np.array(test_item_indices)\n",
    "test_actual_ratings = np.array(test_actual_ratings)\n",
    "\n",
    "# Making predictions\n",
    "test_predictions = mf_model.predict([test_user_indices, test_item_indices]).flatten()\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = sqrt(mean_squared_error(test_actual_ratings, test_predictions))\n",
    "print('Test RMSE:', rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd13b175",
   "metadata": {},
   "source": [
    "### Using Keras with Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1eb78b9",
   "metadata": {},
   "source": [
    "To implement a deep learning-based recommender system using Keras, we can design a neural network that takes user and item embeddings as input and processes them through one or more dense layers. We can also incorporate batch normalization and dropout for regularization and to improve model performance. Here's a step-by-step approach to building such a model:\n",
    "\n",
    "Create Embeddings for Users and Items: Like before, start with embedding layers for users and items.\n",
    "\n",
    "Neural Network Layers: After getting the embeddings, we'll pass them through one or more dense (fully connected) layers. We can experiment with the number and size of these layers.\n",
    "\n",
    "Batch Normalization: Apply batch normalization after each dense layer to normalize the activations and speed up training.\n",
    "\n",
    "Dropout: Use dropout for regularization to prevent overfitting. Dropout randomly sets a fraction of input units to 0 at each update during training time.\n",
    "\n",
    "Output Layer: The final layer should output a single value, the predicted rating.\n",
    "\n",
    "Compile the Model: Choose an appropriate loss function and optimizer.\n",
    "\n",
    "Train the Model: Fit the model to the training data.\n",
    "\n",
    "Evaluate the Model: Assess the model's performance on the test set.\n",
    "\n",
    "Let's write the code for this architecture. Note that the structure and size of the network can be adjusted depending on the specifics of the dataset and the task.\n",
    "\n",
    "Error analyzing\n",
    "As before, we're facing the limitation of not having TensorFlow and Keras installed in this environment. However, the provided code snippet outlines the correct approach to building a deep learning-based recommender system using Keras.\n",
    "\n",
    "Here's a summary of the model architecture and training process:\n",
    "\n",
    "Model Architecture:\n",
    "\n",
    "Input Layers: Separate inputs for users and items.\n",
    "Embedding Layers: Embeddings for users and items.\n",
    "Flatten and Concatenate: Flatten the embeddings and concatenate them.\n",
    "Dense Layers: One or more dense layers with ReLU activation. The number of neurons and layers can be tuned.\n",
    "Batch Normalization: Applied after each dense layer.\n",
    "Dropout: Applied after each dense layer for regularization.\n",
    "Output Layer: A single neuron with linear activation to predict the rating.\n",
    "Model Compilation: The model is compiled with mean squared error as the loss function and the Adam optimizer.\n",
    "\n",
    "Training: The model is trained on the training data (user indices, item indices, and ratings) for a specified number of epochs and batch size.\n",
    "\n",
    "Prediction and Evaluation: After training, you can use the model to predict ratings on the test set and evaluate its performance using metrics like RMSE.\n",
    "\n",
    "To run this code, you need to have TensorFlow and Keras installed in your Python environment. You can install TensorFlow (which includes Keras) using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cc15ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 17.7713\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 16.5449\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 11.9709\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 7.2800\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 6.7957\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 5.2859\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 11.2176\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 12.3750\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 10.5441\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 10.9944\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 11.1406\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 11.8244\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 14.4713\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 11.4226\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 10.6294\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 6.5636\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 10.9130\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 7.6817\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 10.7551\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 8.3216\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 9.0214\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 13.2773\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 12.0517\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 6.3622\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 11.2709\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 9.1848\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 10.3267\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 11.1438\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 10.8013\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 6.7249\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 6.6836\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 9.9508\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 9.1584\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 5.8011\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 8.6278\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 6.6739\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 7.7612\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 8.9571\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 13.2072\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 7.0582\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 8.3959\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 6.4133\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 8.4563\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 13.6359\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 6.7044\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 8.0263\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 8.7769\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 7.4790\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 13.6504\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 7.0626\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 5.4914\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 8.5347\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 10.5533\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 9.8291\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 9.2339\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 7.9237\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 8.1885\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 5.7434\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 13.2631\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 7.8708\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 5.2379\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 6.3007\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 10.4334\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 6.5749\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 7.5824\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.1587\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 6.4832\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 3.1923\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 7.8761\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 5.9695\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 8.9143\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 7.8561\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 6.8226\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4.4854\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 5.6589\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4.5688\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 5.6124\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 10.1026\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 9.2576\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 7.0117\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 9.9115\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 7.5547\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 5.4077\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 5.3472\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 5.5099\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 7.2657\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 12.6100\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.5196\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 5.0315\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 9.3788\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 3.0203\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.8809\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 4.1725\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 5.2015\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 5.3060\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4.0899\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 8.6121\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.2725\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 5.6717\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 6.4001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1a31d015fd0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "\n",
    "def build_deep_learning_model(num_users, num_items, embedding_size):\n",
    "    # User and Item input layers\n",
    "    user_input = Input(shape=(1,), name='user_input')\n",
    "    item_input = Input(shape=(1,), name='item_input')\n",
    "\n",
    "    # User and Item embedding layers\n",
    "    user_embedding = Embedding(output_dim=embedding_size, input_dim=num_users, name='user_embedding')(user_input)\n",
    "    item_embedding = Embedding(output_dim=embedding_size, input_dim=num_items, name='item_embedding')(item_input)\n",
    "\n",
    "    # Flatten the embeddings\n",
    "    user_vector = Flatten()(user_embedding)\n",
    "    item_vector = Flatten()(item_embedding)\n",
    "\n",
    "    # Concatenate user and item vectors\n",
    "    concatenated = keras.layers.Concatenate()([user_vector, item_vector])\n",
    "\n",
    "    # Neural network layers\n",
    "    dense = Dense(128, activation='relu')(concatenated)\n",
    "    dense = BatchNormalization()(dense)\n",
    "    dense = Dropout(0.5)(dense)\n",
    "\n",
    "    dense = Dense(64, activation='relu')(dense)\n",
    "    dense = BatchNormalization()(dense)\n",
    "    dense = Dropout(0.5)(dense)\n",
    "\n",
    "    # Output layer\n",
    "    rating_prediction = Dense(1, activation='linear')(dense)\n",
    "\n",
    "    # Create and compile the model\n",
    "    model = Model(inputs=[user_input, item_input], outputs=rating_prediction)\n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam())\n",
    "\n",
    "    return model\n",
    "\n",
    "# Building the deep learning model\n",
    "dl_model = build_deep_learning_model(num_users, num_items, embedding_size)\n",
    "\n",
    "# Training the deep learning model\n",
    "dl_model.fit([train_user_indices, train_item_indices], train_ratings, epochs=100, verbose=1, batch_size=32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "591f35f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 38ms/step\n",
      "Test RMSE: 2.664883849711728\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Making predictions on the test set\n",
    "test_predictions = dl_model.predict([test_user_indices, test_item_indices]).flatten()\n",
    "\n",
    "# Calculate RMSE on the test set\n",
    "rmse = sqrt(mean_squared_error(test_actual_ratings, test_predictions))\n",
    "print('Test RMSE:', rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bae20e6",
   "metadata": {},
   "source": [
    "## Implementing with a Residual Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3e9d10",
   "metadata": {},
   "source": [
    "Implementing matrix factorization with a residual network in a recommender system is an interesting approach. The idea is to combine the concept of matrix factorization, which captures the linear relationships in user-item interactions, with a residual network (ResNet) architecture, which can capture more complex, non-linear interactions. Here's how you can implement this in Keras:\n",
    "\n",
    "Matrix Factorization Base: Start with the basic matrix factorization model that predicts ratings as the dot product of user and item embeddings.\n",
    "\n",
    "Residual Blocks: Add one or more residual blocks. Each block should have two or more dense layers with a skip connection that adds the input of the block to its output.\n",
    "\n",
    "Combine Outputs: Combine the output of the matrix factorization part and the residual blocks. This can be done by adding them or concatenating them, followed by one or more dense layers to produce the final rating prediction.\n",
    "\n",
    "Compile and Train: Compile the model with an appropriate loss function and optimizer, then train it on your dataset.\n",
    "\n",
    "Evaluation: Evaluate the model on the test set.\n",
    "\n",
    "Let's write the code to build this model. The specifics of the model (like the number of residual blocks and the number of neurons in each layer) can be adjusted depending on your dataset and requirements. Since TensorFlow and Keras are not available in this environment, I'll provide a code outline to illustrate the concept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cc9fe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_residual_matrix_factorization_model(num_users, num_items, embedding_size):\n",
    "    # User and Item input layers\n",
    "    user_input = Input(shape=(1,), name='user_input')\n",
    "    item_input = Input(shape=(1,), name='item_input')\n",
    "\n",
    "    # User and Item embedding layers\n",
    "    user_embedding = Embedding(output_dim=embedding_size, input_dim=num_users, name='user_embedding')(user_input)\n",
    "    item_embedding = Embedding(output_dim=embedding_size, input_dim=num_items, name='item_embedding')(item_input)\n",
    "\n",
    "    # Flatten the embeddings\n",
    "    user_vector = Flatten()(user_embedding)\n",
    "    item_vector = Flatten()(item_embedding)\n",
    "\n",
    "    # Matrix factorization part\n",
    "    mf_part = Dot(axes=1)([user_vector, item_vector])\n",
    "\n",
    "    # Residual block\n",
    "    res_input = Concatenate()([user_vector, item_vector])\n",
    "    # Ensure the first dense layer in the residual block outputs the same shape as res_input\n",
    "    res_block = Dense(res_input.shape[1], activation='relu')(res_input)\n",
    "    res_block = Dense(64, activation='relu')(res_block)\n",
    "    res_block = Dense(res_input.shape[1], activation='relu')(res_block)  # Match the shape\n",
    "    res_block = Add()([res_block, res_input])  # Skip connection\n",
    "\n",
    "    # Combine MF and ResNet parts\n",
    "    combined = Concatenate()([mf_part, res_block])\n",
    "    final_output = Dense(1, activation='linear')(combined)\n",
    "\n",
    "    # Create and compile the model\n",
    "    model = Model(inputs=[user_input, item_input], outputs=final_output)\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb7b53bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " user_input (InputLayer)     [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " item_input (InputLayer)     [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " user_embedding (Embedding)  (None, 1, 20)                20000     ['user_input[0][0]']          \n",
      "                                                                                                  \n",
      " item_embedding (Embedding)  (None, 1, 20)                10000     ['item_input[0][0]']          \n",
      "                                                                                                  \n",
      " flatten_10 (Flatten)        (None, 20)                   0         ['user_embedding[0][0]']      \n",
      "                                                                                                  \n",
      " flatten_11 (Flatten)        (None, 20)                   0         ['item_embedding[0][0]']      \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate  (None, 40)                   0         ['flatten_10[0][0]',          \n",
      " )                                                                   'flatten_11[0][0]']          \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 40)                   1640      ['concatenate_2[0][0]']       \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, 64)                   2624      ['dense_5[0][0]']             \n",
      "                                                                                                  \n",
      " dense_7 (Dense)             (None, 40)                   2600      ['dense_6[0][0]']             \n",
      "                                                                                                  \n",
      " dot_3 (Dot)                 (None, 1)                    0         ['flatten_10[0][0]',          \n",
      "                                                                     'flatten_11[0][0]']          \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 40)                   0         ['dense_7[0][0]',             \n",
      "                                                                     'concatenate_2[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate  (None, 41)                   0         ['dot_3[0][0]',               \n",
      " )                                                                   'add_1[0][0]']               \n",
      "                                                                                                  \n",
      " dense_8 (Dense)             (None, 1)                    42        ['concatenate_3[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 36906 (144.16 KB)\n",
      "Trainable params: 36906 (144.16 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have the function `build_residual_matrix_factorization_model` defined as previously discussed\n",
    "\n",
    "# Number of users and items (for example purposes, replace with actual numbers from your dataset)\n",
    "num_users = 1000  # Example number, replace with actual number of users in your dataset\n",
    "num_items = 500   # Example number, replace with actual number of items in your dataset\n",
    "embedding_size = 20  # Example embedding size, you can adjust this\n",
    "\n",
    "# Step 1: Build the model\n",
    "model = build_residual_matrix_factorization_model(num_users, num_items, embedding_size)\n",
    "\n",
    "# Step 2: Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Optional Step 3: Test a training cycle (requires training data)\n",
    "# model.fit([train_user_indices, train_item_indices], train_ratings, epochs=1, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d664150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.2154\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 10.1024\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 9.9875\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 9.8685\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 9.7450\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 9.6175\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 9.4865\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 9.3558\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 9.2237\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 9.0897\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "Test RMSE: 3.240558979726526\n",
      "Test MAE: 2.7789201935132346\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "\n",
    "# Assuming you have training data: train_user_indices, train_item_indices, train_ratings\n",
    "# And test data: test_user_indices, test_item_indices, test_actual_ratings\n",
    "\n",
    "# Fit the model\n",
    "model.fit([train_user_indices, train_item_indices], train_ratings, epochs=10, batch_size=32, verbose=1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = model.predict([test_user_indices, test_item_indices]).flatten()\n",
    "\n",
    "# Evaluate the model\n",
    "# Calculate RMSE\n",
    "rmse = sqrt(mean_squared_error(test_actual_ratings, test_predictions))\n",
    "print('Test RMSE:', rmse)\n",
    "\n",
    "# Calculate MAE\n",
    "mae = mean_absolute_error(test_actual_ratings, test_predictions)\n",
    "print('Test MAE:', mae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a2d296",
   "metadata": {},
   "source": [
    "### Use of Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dde5974",
   "metadata": {},
   "source": [
    "Using autoencoders for a recommender system is an effective approach, especially for handling sparse and high-dimensional data like user-item interaction matrices. Autoencoders can learn to compress the user-item matrix into a lower-dimensional space (encoding) and then reconstruct it (decoding), which helps in capturing the underlying patterns in the data.\n",
    "\n",
    "Here's a step-by-step guide on how to implement an autoencoder for a recommender system:\n",
    "\n",
    "Data Preparation:\n",
    "\n",
    "The input to the autoencoder is typically the user-item interaction matrix.\n",
    "Normalize or scale the data if necessary.\n",
    "Autoencoder Architecture:\n",
    "\n",
    "Encoder: This part of the network compresses the input into a lower-dimensional representation. It typically consists of one or more dense layers, with the dimensionality decreasing in each subsequent layer.\n",
    "Bottleneck: This layer represents the compressed knowledge of the input data â€” the encoded representation.\n",
    "Decoder: The decoder part attempts to reconstruct the input data from the encoded representation. It usually mirrors the encoder in reverse.\n",
    "Model Training:\n",
    "\n",
    "The autoencoder is trained by feeding it the user-item matrix and setting the target output to be the same as the input.\n",
    "The loss function measures how well the autoencoder can reconstruct the input. Mean squared error is a common choice.\n",
    "Making Predictions:\n",
    "\n",
    "For recommendations, you can use the output of the decoder. It represents the reconstructed user-item matrix with predictions for the missing items.\n",
    "Evaluation:\n",
    "\n",
    "Evaluate the quality of the reconstructed matrix using appropriate metrics, such as RMSE or precision at k.\n",
    "Here's a basic example in Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "736b91b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_id    1    2    3    4\n",
      "user_id                    \n",
      "1        5.0  3.0  0.0  0.0\n",
      "2        4.0  0.0  2.0  0.0\n",
      "3        0.0  1.0  5.0  4.0\n",
      "Epoch 1/50\n",
      "1/1 [==============================] - 1s 945ms/step - loss: 6.3020\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.2436\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 6.1856\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 6.1282\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 6.0716\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 6.0161\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 5.9618\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 5.9086\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 5.8568\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 5.8065\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5.7575\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 5.7103\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 5.6647\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5.6210\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 5.5792\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 5.5394\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 5.5015\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 5.4654\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5.4313\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5.3990\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 5.3684\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.3396\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5.3122\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 5.2864\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 5.2622\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 5.2393\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 5.2179\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 5.1977\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 5.1788\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 5.1610\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 5.1442\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 5.1284\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 5.1135\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 5.0995\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5.0863\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5.0737\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 5.0619\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 5.0506\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 5.0400\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 5.0298\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 5.0200\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 5.0107\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 5.0018\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 4.9932\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 4.9849\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4.9768\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4.9691\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4.9615\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4.9542\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 4.9470\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1a31ff1e8d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "data = {\n",
    "    'user_id': [1, 1, 2, 2, 3, 3, 3],\n",
    "    'item_id': [1, 2, 1, 3, 2, 3, 4],\n",
    "    'rating': [5, 3, 4, 2, 1, 5, 4]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Creating the user-item matrix\n",
    "user_item_matrix = df.pivot_table(index='user_id', columns='item_id', values='rating')\n",
    "\n",
    "# Fill missing values with 0 (assuming 0 means no interaction)\n",
    "user_item_matrix = user_item_matrix.fillna(0)\n",
    "\n",
    "print(user_item_matrix)\n",
    "\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "def build_autoencoder(input_dim, encoding_dim):\n",
    "    # Input\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    \n",
    "    # Encoder\n",
    "    encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "    \n",
    "    # Decoder\n",
    "    decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "    \n",
    "    # Autoencoder\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    \n",
    "    # Encoder model (for later use)\n",
    "    encoder = Model(input_layer, encoded)\n",
    "\n",
    "    autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return autoencoder, encoder\n",
    "\n",
    "# Assuming you have a user-item matrix 'user_item_matrix'\n",
    "input_dim = user_item_matrix.shape[1]  # number of items\n",
    "encoding_dim = 128  # size of the encoding\n",
    "\n",
    "autoencoder, encoder = build_autoencoder(input_dim, encoding_dim)\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(user_item_matrix, user_item_matrix, epochs=50, batch_size=256, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbf500e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 4.1656\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4.1582\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4.1504\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 4.1424\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 4.1343\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4.1261\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4.1178\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 4.1095\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 4.1012\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4.0930\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 4.0849\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4.0769\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 4.0691\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 4.0614\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 4.0539\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 4.0467\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4.0396\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 4.0328\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 4.0262\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4.0198\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4.0137\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4.0078\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 4.0021\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 3.9967\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.9914\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 3.9864\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.9815\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 3.9768\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3.9722\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 3.9678\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3.9636\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 3.9595\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 3.9555\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3.9516\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3.9478\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3.9441\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.9405\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 3.9370\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 3.9335\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.9302\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 3.9269\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 3.9236\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 3.9204\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.9173\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 3.9142\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.9112\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 3.9082\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 3.9053\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3.9025\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.8997\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "Test RMSE: 2.5722194130076885\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Example to create a test set (replace this with your actual test set creation)\n",
    "# Assuming 'user_item_matrix' is your complete dataset\n",
    "train_set = user_item_matrix.sample(frac=0.8, random_state=123)  # 80% for training\n",
    "test_set = user_item_matrix.drop(train_set.index)  # Remaining 20% for testing\n",
    "\n",
    "# Train the autoencoder (assuming autoencoder is your model)\n",
    "autoencoder.fit(train_set, train_set, epochs=50, batch_size=256, shuffle=True)\n",
    "\n",
    "# Predict on the test set\n",
    "test_predictions = autoencoder.predict(test_set)\n",
    "\n",
    "# Flatten the test data and predictions to compute RMSE\n",
    "actual_values = test_set.values.flatten()\n",
    "predicted_values = test_predictions.flatten()\n",
    "\n",
    "# Compute RMSE\n",
    "rmse = sqrt(mean_squared_error(actual_values, predicted_values))\n",
    "print('Test RMSE:', rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4aa067ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, reconstruction error: 6.0\n",
      "Epoch: 1, reconstruction error: 6.75\n",
      "Epoch: 2, reconstruction error: 7.166666507720947\n",
      "Epoch: 3, reconstruction error: 4.916666507720947\n",
      "Epoch: 4, reconstruction error: 5.583333492279053\n",
      "Epoch: 5, reconstruction error: 5.166666507720947\n",
      "Epoch: 6, reconstruction error: 4.916666507720947\n",
      "Epoch: 7, reconstruction error: 5.583333492279053\n",
      "Epoch: 8, reconstruction error: 6.083333492279053\n",
      "Epoch: 9, reconstruction error: 5.5\n",
      "tf.Tensor(\n",
      "[[1. 1. 1. 1.]\n",
      " [1. 0. 1. 1.]\n",
      " [1. 1. 1. 1.]], shape=(3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class RBM:\n",
    "    def __init__(self, visible_dim, hidden_dim):\n",
    "        # Initialize parameters with a specific data type (e.g., tf.float32)\n",
    "        self.W = tf.Variable(tf.random.normal(shape=[visible_dim, hidden_dim], mean=0.0, stddev=0.01, dtype=tf.float32))\n",
    "        self.bh = tf.Variable(tf.zeros([hidden_dim], dtype=tf.float32))\n",
    "        self.bv = tf.Variable(tf.zeros([visible_dim], dtype=tf.float32))\n",
    "\n",
    "    def sample_hidden(self, X):\n",
    "        # Cast X to the same data type as self.W if necessary\n",
    "        X = tf.cast(X, dtype=tf.float32)\n",
    "        h_prob = tf.nn.sigmoid(tf.matmul(X, self.W) + self.bh)\n",
    "        h_sample = tf.nn.relu(tf.sign(h_prob - tf.random.uniform(tf.shape(h_prob), dtype=tf.float32)))\n",
    "        return h_sample\n",
    "\n",
    "    def sample_visible(self, h):\n",
    "        v_prob = tf.nn.sigmoid(tf.matmul(h, tf.transpose(self.W)) + self.bv)\n",
    "        v_sample = tf.nn.relu(tf.sign(v_prob - tf.random.uniform(tf.shape(v_prob))))\n",
    "        return v_sample\n",
    "\n",
    "    def train(self, X, learning_rate=0.01, batch_size=100, epochs=10):\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                batch_x = X[i:i+batch_size]\n",
    "\n",
    "                # Contrastive Divergence\n",
    "                h_sample = self.sample_hidden(batch_x)\n",
    "                v_sample = self.sample_visible(h_sample)\n",
    "\n",
    "                positive_grad = tf.matmul(tf.transpose(batch_x), h_sample)\n",
    "                negative_grad = tf.matmul(tf.transpose(v_sample), h_sample)\n",
    "\n",
    "                # Update parameters\n",
    "                self.W.assign_add(learning_rate * (positive_grad - negative_grad) / tf.dtypes.cast(tf.shape(batch_x)[0], tf.float32))\n",
    "                self.bv.assign_add(learning_rate * tf.reduce_mean(batch_x - v_sample, 0))\n",
    "                self.bh.assign_add(learning_rate * tf.reduce_mean(h_sample, 0))\n",
    "\n",
    "            # Monitor training progress\n",
    "            error = tf.reduce_mean(tf.square(batch_x - v_sample))\n",
    "            print(f'Epoch: {epoch}, reconstruction error: {error.numpy()}')\n",
    "\n",
    "    def predict(self, X):\n",
    "        h = self.sample_hidden(X)\n",
    "        reconstructed_X = self.sample_visible(h)\n",
    "        return reconstructed_X\n",
    "\n",
    "# Assuming you have a user-item interaction matrix 'user_item_matrix'\n",
    "visible_dim = user_item_matrix.shape[1]  # Number of items\n",
    "hidden_dim = 64  # Can be tuned\n",
    "\n",
    "rbm = RBM(visible_dim, hidden_dim)\n",
    "user_item_matrix = tf.cast(user_item_matrix, dtype=tf.float32)\n",
    "rbm.train(user_item_matrix, learning_rate=0.01, epochs=10)\n",
    "\n",
    "# Predict ratings for a user (or a batch of users)\n",
    "predicted_ratings = rbm.predict(user_item_matrix)\n",
    "print(predicted_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de472fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
